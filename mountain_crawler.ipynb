{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6a20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력해주세요:산악사고\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):1\n",
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):2\n",
      "\n",
      "크롤링할 종료 페이지:  2 페이지\n",
      "생성url:  ['https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악사고&start=1', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악사고&start=11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#크롤링시 필요한 라이브러리 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "  #입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "\n",
    "# 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "\n",
    "def makeUrl(search, start_pg, end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page)\n",
    "        print(\"생성url: \", url)\n",
    "        return url\n",
    "    else:\n",
    "        urls = []\n",
    "        for i in range(start_pg, end_pg + 1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \", urls)\n",
    "        return urls    \n",
    "\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "\n",
    "# ConnectionError방지\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "\n",
    "#html생성해서 기사크롤링하는 함수 만들기(url): 링크를 반환\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(i,headers=headers)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "\n",
    "    url_naver = html.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    url = news_attrs_crawler(url_naver,'href')\n",
    "    return url\n",
    "#####뉴스크롤링 시작#####\n",
    "\n",
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "#검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "#검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")   \n",
    "\n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search,page,page2)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "news_dates = []\n",
    "for i in url:\n",
    "    url = articles_crawler(url)\n",
    "    news_url.append(url)\n",
    "\n",
    "\n",
    "#제목, 링크, 내용 1차원 리스트로 꺼내는 함수 생성\n",
    "def makeList(newlist, content):\n",
    "    for i in content:\n",
    "        for j in i:\n",
    "            newlist.append(j)\n",
    "    return newlist\n",
    "\n",
    "    \n",
    "#제목, 링크, 내용 담을 리스트 생성\n",
    "news_url_1 = []\n",
    "\n",
    "#1차원 리스트로 만들기(내용 제외)\n",
    "makeList(news_url_1,news_url)\n",
    "\n",
    "#NAVER 뉴스만 남기기\n",
    "final_urls = []\n",
    "for i in tqdm(range(len(news_url_1))):\n",
    "    if \"news.naver.com\" in news_url_1[i]:\n",
    "        final_urls.append(news_url_1[i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8484db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력해주세요:산악사고\n",
      "\n",
      "크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):1\n",
      "\n",
      "크롤링할 시작 페이지:  1 페이지\n",
      "\n",
      "크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):2\n",
      "\n",
      "크롤링할 종료 페이지:  2 페이지\n",
      "생성url:  ['https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악사고&start=1', 'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=산악사고&start=11']\n",
      "검색된 기사 갯수: 총  20 개\n",
      "\n",
      "[뉴스 제목]\n",
      "[['봄기운 즐기려다 실족 추락...봄철 산악사고 주의보', '봄기운 즐기려다 삐끗... 산악사고 주의보', '화창한 날씨 충북서 산악사고 잇따라…\"안전수칙 준수\"', '부산 향하던 산악회 버스 고장...42명 갓길 대피', '포항북부소방서 119구조대, 산악사고 대비 구조훈련 실시', \"산 정상서 '인생샷' 남기려다 미끌… 봄철 산악사고 주의보\", '[독자기고] 산악사고 예방 및 대처요령', '공단소방서, 상반기 산악사고 대비 산악구조훈련', '산악 어드벤처 종합세트, 유격 훈련처럼 오르는 산', '국립공원공단, 산악단체와 국립공원 암벽장 합동점검 실시'], [\"진천소방서, 산악사고 대비 '산악위치표지판' 안전점검\", \"경기도특수대응단 '봄철 산악사고' 대비 합동훈련\", '평창 캠핑장서 승용차 10m 아래 추락…어린이날 연휴 사고 속출', '국립공원공단, 산악단체와 암벽장 합동 안전점검', '전북소방,산악구조 훈련 늘려 인명피해 줄인다', '산악 어드벤처 종합세트, 유격 훈련처럼 오르는 산', '네팔 동부 산악지대서 헬기 추락…\"1명 사망·4명 부상\"', \"강원도 산악사고 헬기구조 67%가 '외지인'…가을과 봄철에 많아\", '국립공원공단, 산악단체와 국립공원 암벽장 합동점검 실시', '국립공원공단과 산악단체 암벽장 합동 안전점검 실시']]\n",
      "\n",
      "[뉴스 링크]\n",
      "[['https://www.ytn.co.kr/_ln/0115_202305140210066190', 'https://www.ccdailynews.com/news/articleView.html?idxno=2204486', 'https://www.yna.co.kr/view/AKR20230512137500064?input=1195m', 'https://www.ytn.co.kr/_ln/0115_202305142247011417', 'https://www.news1.kr/articles/5043417', 'https://hankookilbo.com/News/Read/A2023050818140005614?did=NA', 'http://www.jemin.com/news/articleView.html?idxno=754968', 'http://fpn119.co.kr/197529', 'http://www.gjdream.com/news/articleView.html?idxno=627507', 'https://news.mtn.co.kr/news-detail/2023051109382268569'], ['http://www.jbnews.com/news/articleView.html?idxno=1393238', 'http://www.newsis.com/view/?id=NISX20230503_0002289555&cID=14001&pID=14000', 'http://www.kwnews.co.kr/page/view/2023050713132698398', 'https://www.pressian.com/pages/articles/2023051018332938140?utm_source=naver&utm_medium=search', 'http://sjbnews.com/news/news.php?number=779500', 'http://www.gjdream.com/news/articleView.html?idxno=627507', 'https://www.yna.co.kr/view/AKR20230506026500077?input=1195m', 'https://www.news1.kr/articles/5035383', 'https://news.mtn.co.kr/news-detail/2023051109382268569', 'https://www.job-post.co.kr/news/articleView.html?idxno=76190']]\n",
      "\n",
      "[뉴스 내용]\n"
     ]
    }
   ],
   "source": [
    "# 페이지 url 형식에 맞게 바꾸어 주는 함수 만들기\n",
    "  #입력된 수를 1, 11, 21, 31 ...만들어 주는 함수\n",
    "def makePgNum(num):\n",
    "    if num == 1:\n",
    "        return num\n",
    "    elif num == 0:\n",
    "        return num+1\n",
    "    else:\n",
    "        return num+9*(num-1)\n",
    "    # 크롤링할 url 생성하는 함수 만들기(검색어, 크롤링 시작 페이지, 크롤링 종료 페이지)\n",
    "def makeUrl(search,start_pg,end_pg):\n",
    "    if start_pg == end_pg:\n",
    "        start_page = makePgNum(start_pg)\n",
    "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(start_page)\n",
    "        print(\"생성url: \",url)\n",
    "        return url\n",
    "    else:\n",
    "        urls= []\n",
    "        for i in range(start_pg,end_pg+1):\n",
    "            page = makePgNum(i)\n",
    "            url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + search + \"&start=\" + str(page)\n",
    "            urls.append(url)\n",
    "        print(\"생성url: \",urls)\n",
    "        return urls\n",
    "# html에서 원하는 속성 추출하는 함수 만들기 (기사, 추출하려는 속성값)\n",
    "def news_attrs_crawler(articles,attrs):\n",
    "    attrs_content=[]\n",
    "    for i in articles:\n",
    "        attrs_content.append(i.attrs[attrs])\n",
    "    return attrs_content\n",
    "#뉴스기사 내용 크롤링하는 함수 만들기(각 뉴스의 url)\n",
    "def news_contents_crawler(news_url):\n",
    "    contents=[]\n",
    "    for i in news_url:\n",
    "        #각 기사 html get하기\n",
    "        news = requests.get(i)\n",
    "        news_html = BeautifulSoup(news.text,\"html.parser\")\n",
    "            #기사 내용 가져오기 (p태그의 내용 모두 가져오기) \n",
    "        contents.append(news_html.find_all('p'))\n",
    "    return contents\n",
    "#html생성해서 기사크롤링하는 함수 만들기(제목,url): 3개의 값을 반환함(제목, 링크, 내용)\n",
    "def articles_crawler(url):\n",
    "    #html 불러오기\n",
    "    original_html = requests.get(i)\n",
    "    html = BeautifulSoup(original_html.text, \"html.parser\")\n",
    "    # 검색결과\n",
    "    articles = html.select(\"div.group_news > ul.list_news > li div.news_area > a\")\n",
    "    title = news_attrs_crawler(articles,'title')\n",
    "    url = news_attrs_crawler(articles,'href')\n",
    "    content = news_contents_crawler(url)\n",
    "    return title, url, content #3개의 값을 반환\n",
    "#뉴스크롤링 시작\n",
    "\n",
    "#검색어 입력\n",
    "search = input(\"검색할 키워드를 입력해주세요:\")\n",
    "\n",
    "#검색 시작할 페이지 입력\n",
    "page = int(input(\"\\n크롤링할 시작 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 시작 페이지: \",page,\"페이지\")   \n",
    "#검색 종료할 페이지 입력\n",
    "page2 = int(input(\"\\n크롤링할 종료 페이지를 입력해주세요. ex)1(숫자만입력):\")) # ex)1 =1페이지,2=2페이지...\n",
    "print(\"\\n크롤링할 종료 페이지: \",page2,\"페이지\")   \n",
    "\n",
    "# naver url 생성\n",
    "url = makeUrl(search,page,page2)\n",
    "\n",
    "#뉴스 크롤러 실행\n",
    "news_titles = []\n",
    "news_url =[]\n",
    "news_contents =[]\n",
    "for i in url:\n",
    "    title, url,content = articles_crawler(url)\n",
    "    news_titles.append(title)\n",
    "    news_url.append(url)\n",
    "    news_contents.append(content)\n",
    "\n",
    "print(\"검색된 기사 갯수: 총 \",(page2+1-page)*10,'개')\n",
    "print(\"\\n[뉴스 제목]\")\n",
    "print(news_titles)\n",
    "print(\"\\n[뉴스 링크]\")\n",
    "print(news_url)\n",
    "print(\"\\n[뉴스 내용]\")\n",
    "###print(news_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
